# LLM Provider Configuration
# ===========================
# Specify the primary LLM provider (default: gemini)
LLM_PRIMARY_PROVIDER=gemini

# Specify fallback providers in order of preference (comma-separated, default: claude)
# If primary provider fails or hits rate limits, these will be tried in order
LLM_FALLBACK_PROVIDERS=claude

# LLM Generation Parameters
# ===========================
# Temperature: Controls randomness (0.0 = deterministic, 1.0 = very random)
# For consistent, factual responses use 0.0
LLM_TEMPERATURE=0.0

# Maximum tokens in the response
LLM_MAX_TOKENS=1024

# Retry Configuration for Rate Limits
# ===========================
# Maximum number of retries when hitting rate limits (429 errors)
LLM_RETRY_MAX_ATTEMPTS=5

# Initial delay in seconds between retries (uses exponential backoff: delay * 2^attempt)
LLM_RETRY_DELAY_SECONDS=5

# API Keys
# ===========================
# Google Gemini API Key
# Get from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your-google-api-key-here

# Anthropic Claude API Key
# Get from: https://console.anthropic.com/account/keys
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Other Configuration
# ===========================
# Gmail Configuration
GMAIL_CREDENTIALS_FILE=credentials.json
GMAIL_TOKEN_FILE=token.json

# Pinecone Configuration
PINECONE_API_KEY=your-pinecone-key-here
PINECONE_INDEX_NAME=customer-support-index
PINECONE_DIMENSION=768
PINECONE_METRIC=cosine
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1

# Telegram Configuration
TELEGRAM_BOT_TOKEN=your-telegram-bot-token-here

# RAG Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=150
EMBEDDING_MODEL=models/embedding-001

# Prompt Files
AGENT_SYSTEM_PROMPT_FILE=prompts/agent_system_prompt.txt
RETRIEVER_TOOL_DESC_FILE=prompts/retriever_tool_description.txt
EMAIL_CLASSIFICATION_PROMPT_FILE=prompts/email_classification_prompt.txt

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/app.log
